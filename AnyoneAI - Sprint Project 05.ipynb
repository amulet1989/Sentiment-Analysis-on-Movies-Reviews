{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wycYqOwt_0KE"
   },
   "source": [
    "# Sentiment Analysis: Large Movie Review Dataset\n",
    "\n",
    "Hi again! You will be expected to finish this on your own, but you can use the available channels on Discord to ask questions and help others. Please read the entire notebook before starting, this will give you a better idea of what you need to accomplish.\n",
    "\n",
    "This project is related to NLP. As you may already know, the most important and hardest part of an NLP project is pre-processing, which is why we are going to focus on that.\n",
    "\n",
    "### Getting the data\n",
    "\n",
    "To access the data for this project, you only need to execute the code below. This will download three files:\n",
    "\n",
    "- `movies_review_train_aai.csv`: Training dataset you must use to train and find the best hyperparameters on your model.\n",
    "\n",
    "- `movies_review_test_aai.csv`: Test dataset to test your model.\n",
    "\n",
    "Basically a basic sentiment analysis problem, as in this case, consists of a classification problem, where the possible output labels are: `positive` and `negative`. Which indicates, if the review of a movie speaks positively or negatively. In our case it is a binary problem, but one could have many more \"feelings\" tagged and thus allow a more granular analysis.\n",
    "\n",
    "### These are the objectives of the project:\n",
    "\n",
    "* Read data that is not in a traditional format.\n",
    "* Put together a set of preprocessing functions that we can use later on any NLP or related problems.\n",
    "* Vectorize the data in order to apply a machine learning model to it: using BoW or TF-IDF.\n",
    "* BoW and TF-IDF are classic ways to vectorize text, but currently we have some more complex ways with better performance, for this we are going to train our own word embedding and use it as a vectorization source for our data.\n",
    "* Train a sentiment analysis model that allows us to detect positive and negative opinions in movie reviews."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0oiYxBSs_0KH"
   },
   "source": [
    "---\n",
    "## 1. Get the data\n",
    "\n",
    "**Download the data by executing the code below:**\n",
    "\n",
    "`Notes:` Use the target column as `positive`, that way the positive value will be indicated with a value of `1` and negative with a value of `0`. In this case, a split train/test is not necessary because the original data is already separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import data_utils\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from src import word2vec\n",
    "from src import evaluation\n",
    "from src import text_normalizer\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "train, test = data_utils.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Complete in this cell: Complete the function `split_data()` but not here in\n",
    "# the notebook, do it in the python module called `data_utils.py`.\n",
    "# Then make sure this code runs without errors.\n",
    "X_train, y_train, X_test, y_test = data_utils.split_data(train, test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M4SOofBj_0KM"
   },
   "source": [
    "---\n",
    "## 2. Normalize the data\n",
    "\n",
    "**Create the following functions but not here in the notebook, do it in the python script called `text_normalizer.py` and import them into the notebook (this way you can build your own NLP preprocessing library). In fact, the structure of the functions is already written, you must complete them with the code that you consider necessary.**\n",
    "\n",
    "- `remove_html_tags(text):` to remove all HTML tags that may be present in text.\n",
    "- `remove_accented_chars(text):` to remove accented characters from text\n",
    "- `expand_contractions(text):` to expand contractions of the type, \"don't\" to \"do not\". The contractions are already defined in the \"contractions.py\" file.\n",
    "- `lemmatize_text(text):` to lemmatize text.\n",
    "- `stem_text(text):` to apply stemming (NLTK's PorterStemmer) on text.\n",
    "- `remove_special_chars(text):` to remove special characters from text.\n",
    "- `remove_special_chars(text, remove_digits=True):` to remove numbers, note that it is the same function to remove special characters with the addition of an argument that enables or disables the removal of numbers.\n",
    "- `remove_stopwords(text, stopwords=stop_words):` to remove stopwords from text.\n",
    "- `remove_extra_new_lines(text):` to remove extra newlines from text.\n",
    "- `remove_extra_whitespace(text):` to remove extra whitespaces from text.\n",
    "\n",
    "If you want to add more features that would be great, for example you could start by removing emojis, using different stemming algorithms, etc. The more functions you have the better, remember that the texts are very varied and the preprocessing depends a lot on the source of our data.\n",
    "\n",
    "To apply each of the functions you created and pre-process the dataset, you must use the `normalize_corpus()` function of the `text_normalizer.py` script. In this method each of the functions you wrote is called, in fact you must enable or disable what you consider necessary (at this point we leave it to your free choice, for example, you can lemmatize or apply stemming or directly not apply any of the two and so on with the rest, but that is your choice), this function simply groups the previous ones for a more simplified use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4rI96sr_0KN"
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJtdCt9d_0KN"
   },
   "outputs": [],
   "source": [
    "# TODO: Complete all the functions with the `TODO` comment inside the\n",
    "# module `text_normalizer.py`. Then make sure this code runs without errors.\n",
    "# You can change the parameters for `normalize_corpus()` if you want.\n",
    "norm_train_reviews = text_normalizer.normalize_corpus(X_train, stopwords=stop_words)\n",
    "norm_test_reviews = text_normalizer.normalize_corpus(X_test, stopwords=stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uVKvNzkw_0KO"
   },
   "source": [
    "**(\\*) Functions will be checked using unit tests.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ilBNdrN1_0KP"
   },
   "source": [
    "---\n",
    "## 3. Feature Engineering\n",
    "\n",
    "You already have the pre-processed data, now you must vectorize them, because remember that the models only understand numbers. At this stage choose whether you want to vectorize with BoW or with TF-IDF. Later we will train our own embedding but for now we go with a more \"classic\" vectorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDAsFB-L_0KP"
   },
   "outputs": [],
   "source": [
    "# TODO Complete in this cell: Use BoW or TF-IDF to vectorize your data.\n",
    "# Remember to call the `fit()` method only on the train dataset!\n",
    "# Assign the features to the variables `train_features` and `test_features`.\n",
    "\n",
    "\n",
    "train_features = ...\n",
    "test_features = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pVEP8DA4_0KQ"
   },
   "source": [
    "## 4. Modeling and Performance Evaluation\n",
    "\n",
    "As we said at the beginning, what interests us most in this part is pre-processing. However, we must train a model, so choose a model of your choice (obviously a classification model, given the problem we are facing) and apply everything we learned. Also if you want you can try several models, the more models you use and know better!\n",
    "\n",
    "**In addition to training the model we ask you to show:**\n",
    "\n",
    "- `Precision`\n",
    "- `Recall`\n",
    "- `F1-Score`\n",
    "- `Classification Report`\n",
    "- `Confusion Matrix`\n",
    "\n",
    "**To do this you must complete the `get_performance` function of the `evaluation.py` script.**\n",
    "\n",
    "**Also, you must complete the `plot_roc` function so that it can show:**\n",
    "\n",
    "- `ROC Curve`\n",
    "- `Obtain the ROC-AUC value (later we will do a small minimum performance check with this value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCiCqub0_0KQ"
   },
   "outputs": [],
   "source": [
    "# TODO Complete in this cell: Create and train your own model.\n",
    "# Having the model trained, use it to make predictions on the test dataset.\n",
    "# Assign the predictions to the variable `model_predictions`, it will be used in the\n",
    "# following cell to evaluate the model performance.\n",
    "\n",
    "baseline_model = ...\n",
    "\n",
    "baseline_model.fit(train_features, y_train)\n",
    "model_predictions = baseline_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ykz1bOKm_0KR",
    "outputId": "b657b8df-33c8-43b5-d97f-18ead07c624d"
   },
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1_score = evaluation.get_performance(\n",
    "    model_predictions, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.plot_roc(baseline_model, y_test, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Classifying using K-Means\n",
    "\n",
    "Let use tfidf features computed in last section as vector inputs for kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Complete in this cell: Train the k-means clustering model using `n_clusters=2`.\n",
    "# Having the model trained, use it to make predictions on the test dataset.\n",
    "# Assign the predictions to the variable `kmeans_predictions`, it will be used in the\n",
    "# following cell to evaluate the model performance.\n",
    "\n",
    "kmeans = ...\n",
    "\n",
    "kmeans.fit(train_features)\n",
    "print(f\"Converged after {kmeans.n_iter_} iterations\")\n",
    "kmeans_predictions = kmeans.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1_score = evaluation.get_performance(\n",
    "    [1 - el for el in kmeans_predictions], y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4pZ2FDV_0KS"
   },
   "source": [
    "---\n",
    "## 6. Feature Engineering with Custom Word Embedding\n",
    "\n",
    "### Tokenize reviews and train your own Word Embedding\n",
    "\n",
    "You are going to have to train your own word embedding, for this we are going to use the __gensim__ library. The only requirement we ask of you is that the $vector\\_size=100$.\n",
    "\n",
    "[Here](https://radimrehurek.com/gensim/models/word2vec.html) you can read Gensim's Word2Vec documentation so you can train your own embedding, using the review data as a corpus.\n",
    "\n",
    "As a previous step to training your word embedding you must tokenize the corpus, this may take a bit depending on the size of the dataset and the tokenizer we use, if you want you can try the NLTK tokenizer called `ToktokTokenizer`, which turns out to be a little faster (we hope that this recommendation does not bias your work, try and use the ones you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nhx6o1pJ_0KS",
    "outputId": "96b56add-1a85-4516-922d-0d4ee0984540"
   },
   "outputs": [],
   "source": [
    "# TODO Complete in this cell: Tokenize your text corpus and use them to train\n",
    "# a Word2Vec model.\n",
    "\n",
    "# TODO: Create and store here the tokenized train and test data.\n",
    "tokenized_train = []\n",
    "tokenized_test = []\n",
    "\n",
    "# TODO: Train your Word2Vec model and assign it to `model_w2v`.\n",
    "# The output model vector size (w2v_vector_size) is set by default to 100,\n",
    "# you can change it if you want.\n",
    "w2v_vector_size = 100\n",
    "model_w2v = ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1yybEN7P_0KS"
   },
   "source": [
    "### Generate averaged word vector features\n",
    "\n",
    "Once the embedding has been trained, we must use it. Remember that embedding will convert each word you pass to it into a vector of a given dimension (in our case $vector\\_size=100$). So in order to obtain a vector for each review, you must average the vectors of all the words that are part of the same review.\n",
    "\n",
    "The function must have the following form:\n",
    "* `vectorizer(corpus, model, num_features=100)`\n",
    "\n",
    "\n",
    "Where:\n",
    "* `corpus:` corresponds to the entire dataset, in this way we obtain an average vector for each review, with a single call to the function.\n",
    "* `model:` is your trained model.\n",
    "* `num_features:` the dimension of the output vector of your embedding (remember that in our case we set this value to 100).\n",
    "\n",
    "To do this you must complete the `vectorize` function of the `word2vec.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DN7oCLhd_0KT"
   },
   "outputs": [],
   "source": [
    "# TODO: Make sure you have completed the `vectorizer()` function from\n",
    "# word2vec.py module.\n",
    "# You don't need to change this code, just make it run without errors.\n",
    "w2v_train_features = word2vec.vectorizer(\n",
    "    corpus=tokenized_train, model=model_w2v, num_features=w2v_vector_size\n",
    ")\n",
    "w2v_test_features = word2vec.vectorizer(\n",
    "    corpus=tokenized_test, model=model_w2v, num_features=w2v_vector_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ekf7XrPd_0KU"
   },
   "source": [
    "### Modeling\n",
    "\n",
    "Finally train a new model, it can be the same one you used before and compare the results you got using BoW/TF-IDF and Word2Vec.\n",
    "\n",
    "In addition to training the model we ask you to show:\n",
    "\n",
    "* `Accuracy`\n",
    "* `Recall`\n",
    "* `F1-Score`\n",
    "* `Classification Report`\n",
    "* `Confusion Matrix`\n",
    "* `ROC Curve`\n",
    "* `Obtain the ROC-AUC value (later we will do a small minimum performance check with this value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxYRc-_8_0KU"
   },
   "outputs": [],
   "source": [
    "# TODO Complete in this cell: Train and choose the best model for the task.\n",
    "# Assign this model to the `best_model` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeFGjvLW_0KU",
    "outputId": "742c841d-b6e5-4a95-a631-3fb1e646897a"
   },
   "outputs": [],
   "source": [
    "# TODO: Use the `get_performance()` function from `evaluation.py` module to show\n",
    "# the model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pes1qmmJ_0KU",
    "outputId": "ca1bc607-507f-4196-d44c-f2843e8e7b01",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use the `plot_roc()` function from `evaluation.py` module to show\n",
    "# the model ROC curve.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8KZdYlRs_0KV"
   },
   "source": [
    "## 7. Predict data\n",
    "\n",
    "- Take your best model\n",
    "- Take `test data` (i.e. the dataset after doing the preprocessing and feature engineering part)\n",
    "- Run the data through your model and save the predictions on the `positive` column in the `test` DataFrame (yeah that we've loaded at the very beginning of this notebook).\n",
    "- You will have to use that model to fill values in the positive column using the model predictions\n",
    "- Save the modified version of the DataFrame with the name (`dataset/movies_review_predict_aai.csv`) and don't forget to submit it alongside the rest of this sprint project code.\n",
    "\n",
    "Let's say your best model is called `logistic_word2vec`, then your code should be exactly this:\n",
    "\n",
    "```python\n",
    "    from src import config\n",
    "    from pathlib import Path\n",
    "    DATASET_TEST_PREDICT = str(Path(config.DATASET_ROOT_PATH) / \"movies_review_predict_aai.csv\")\n",
    "    test_preds = logistic_word2vec.predict_proba(w2v_test_features)[:, 1]\n",
    "    test[\"positive\"] = test_preds\n",
    "    test.to_csv(DATASET_TEST_PREDICT, index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkopeTCj_0KV"
   },
   "source": [
    "---\n",
    "### OPTIONAL:\n",
    "\n",
    "In our case, we train a word embedding from scratch, which is very good at an educational level, but when applying it to a real problem, we need a lot of data (which is not the case with our problem). Therefore, we invite you to investigate and use one of the `pre-trained Word2Vec models`.\n",
    "\n",
    "If you look for the `Pretrained models` section in this [link](https://radimrehurek.com/gensim/models/word2vec.html), you will find information about the models that Gensim owns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment_Analysis_NLP_Solved.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
